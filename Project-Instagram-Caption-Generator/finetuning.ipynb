{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning using Transformers\n",
    "In this jupter notebook, I will first use a pre-trained model to build up the image captioning model. Then I will fine-tune the model using the **Instagram Images with Captions** dataset from Kaggle. \n",
    "\n",
    "The fine-tuning will be done twice for each selected pre-trained model. The first fine-tuning will be done only on the last several layers in the selected model, which means I need to freeze all the prior layers. The second fine-tuning will be done on all the parameters in the selected model.\n",
    "\n",
    "### Model Selection\n",
    "1. encoder-decoder model which has not been fine-tuned for image captioning. For the image_encoder_model = **\"google/vit-base-patch16-224-in21k\"** and text_decoder_model = **\"gpt2\"**.\n",
    "2. encoder-decoder model which has been fine-tuned for image captioning. For the image_encoder_model = **\"nlpconnect/vit-gpt2-image-captioning\"** and text_decoder_model = **\"nlpconnect/vit-gpt2-image-captioning\"**.\n",
    "\n",
    "References:\n",
    "- https://ankur3107.github.io/blogs/the-illustrated-image-captioning-using-transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, GPT2TokenizerFast, ViTImageProcessor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "from transformers import default_data_collator\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert my customerized dataset to HuggingFace's dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"fine_tuning_dataset/instagram_data/captions_csv.csv\",  names=[\"id\", \"image_path\", \"caption\"],nrows=5000,header =1)\n",
    "\n",
    "# Drop rows where the 'Caption' column is empty\n",
    "df = df.dropna(subset=['caption'])\n",
    "\n",
    "dataset = Dataset.from_pandas(df[['caption', 'image_path']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['caption', 'image_path', '__index_level_0__'],\n",
       "    num_rows: 4054\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the valid rows of the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c584477c93407cb3348cd840e01441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_image_with_caption(example):\n",
    "    image_path = f\"fine_tuning_dataset/instagram_data/{example['image_path']}.jpg\"\n",
    "    with Image.open(image_path) as image:\n",
    "        image = image.convert(\"RGB\")\n",
    "    return {'image': image, 'caption': example['caption']}\n",
    "\n",
    "# Map the function over the dataset\n",
    "dataset = dataset.map(load_image_with_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['caption', 'image_path', '__index_level_0__', 'image'],\n",
       "    num_rows: 4054\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0648a038574802a1c84dee552f6a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/9 shards):   0%|          | 0/4054 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the dataset to disk\n",
    "dataset.save_to_disk('raw_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in 'caption': 0\n"
     ]
    }
   ],
   "source": [
    "# Check no null values in the caption column\n",
    "# This step is CRITICAL for the future step to work\n",
    "null_count = sum(1 for caption in dataset['caption'] if caption is None)\n",
    "print(f\"Number of nulls in 'caption': {null_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As the feature extractor and tokenizer will be the same for all the following fine-tuning, I will first convert the dataset to the format that model can read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "# image feature extractor\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(image_encoder_model)\n",
    "# text tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(text_decode_model)\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing step\n",
    "def tokenization_fn(caption):\n",
    "    \"\"\"Run tokenization on captions.\"\"\"\n",
    "    labels = tokenizer(caption, \n",
    "                      max_length=128,\n",
    "                      padding=\"max_length\",\n",
    "                      truncation=True).input_ids # Must explicitly enable truncation\n",
    "\n",
    "    return labels\n",
    "\n",
    "# image preprocessing step\n",
    "def feature_extraction_fn(image):\n",
    "    encoder_inputs = feature_extractor(image, return_tensors=\"np\").pixel_values\n",
    "    return encoder_inputs\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    \"\"\"Run tokenization + image feature extraction\"\"\"\n",
    "    image = examples['image']\n",
    "    caption = examples['caption']\n",
    "    \n",
    "    model_inputs = {}\n",
    "\n",
    "    model_inputs['labels'] = tokenization_fn(caption)\n",
    "    model_inputs['pixel_values'] = feature_extraction_fn(image)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "986c293855914d6a9282364d848aa956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff8ea0fb0d748799690190a86f5f72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the dataset into train and test\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "processed_dataset = DatasetDict()\n",
    "\n",
    "processed_dataset['train'] = train_test_split['train'].map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=train_test_split['train'].column_names\n",
    ")\n",
    "\n",
    "processed_dataset['test'] = train_test_split['test'].map(\n",
    "    function=preprocess_fn,\n",
    "    batched=True,\n",
    "    remove_columns=train_test_split['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c86ca8dbadf4b83a2f2cb8d9c573a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/3243 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7621fa5a93c4fa4af874480725ed756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the dataset to disk\n",
    "processed_dataset.save_to_disk('processed_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_dataset = DatasetDict.load_from_disk('processed_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, try encoder-decoder model which has not been fine-tuned for image captioning\n",
    "- image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "- text_decode_model = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built())\n",
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_built() else torch.device(\"cpu\")  # mps is for Apple Silicon GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.11.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.10.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.4.ln_cross_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.6.ln_cross_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.4.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.1.ln_cross_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.0.ln_cross_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.10.ln_cross_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.11.ln_cross_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.8.ln_cross_attn.weight', 'h.2.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.9.ln_cross_attn.weight', 'h.6.ln_cross_attn.bias', 'h.1.ln_cross_attn.bias', 'h.6.crossattention.c_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.bias', 'h.2.ln_cross_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.7.crossattention.c_attn.weight', 'h.8.ln_cross_attn.bias', 'h.3.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.bias', 'h.10.ln_cross_attn.weight', 'h.3.ln_cross_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.7.ln_cross_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.bias', 'h.9.ln_cross_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.5.ln_cross_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.0.ln_cross_attn.bias', 'h.3.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.5.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.4.crossattention.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "text_decode_model = \"gpt2\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    image_encoder_model, text_decode_model)\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239,195,904 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# Check the number of parameters of the model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate caption on sample images without fine-tuning on IG dataset\n",
    "- image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "- text_decode_model = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a inference function\n",
    "def generate_caption(image_folder):\n",
    "    '''\n",
    "    Use the ABSOLUTE path of the image folder\n",
    "    '''\n",
    "    generated_texts = []\n",
    "\n",
    "    image_paths = glob.glob(os.path.join(image_folder, '*'))\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        with Image.open(image_path) as image:\n",
    "            pixel_values = feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
    "            pixel_values = pixel_values.to(device)  # Move pixel values to GPU device\n",
    "\n",
    "            generated_ids = model.generate(pixel_values,max_length=128)\n",
    "            generated_ids = generated_ids.to(device)   # Move input_ids/labels to GPU device\n",
    "\n",
    "            generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            generated_texts.append(generated_text)\n",
    "\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2501: UserWarning: MPS: no support for int64 for min_max, downcasting to a smaller data type (int32/float32). Native support for int64 has been added in macOS 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:621.)\n",
      "  if unfinished_sequences.max() == 0:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nThe first time I saw the new version of the game, I was so excited. I',\n",
       " '\\nThe first time I saw the new version of the game, I was so excited. I',\n",
       " '\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '\\nThe first time I saw the new version of the game, I was so excited. I',\n",
       " '\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '\\nThe first time I saw the new version of the game, I was so excited. I',\n",
       " '\\nThe U.S. Department of Justice has filed a lawsuit against the company that owns the',\n",
       " '\\nThe first time I saw the new version of the game, I was so excited. I',\n",
       " '\\nThe first time I saw the new version of the game, I was so excited. I',\n",
       " '\\nThe first time I saw the new version of the game, I was so excited. I',\n",
       " '\\nThe first time I saw the new version of the game, I was so excited. I']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_folder = \"/Users/stoneman/Library/CloudStorage/OneDrive-Vanderbilt/Transformers/Transformers/Final-Project-Automatic-IG-Caption-Generator/test_images\"\n",
    "generate_caption(image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "- image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "- text_decode_model = \"gpt2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune only the last several layers in the selected model (freeze all the prior layers)\n",
    "- image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "- text_decode_model = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, freeze all parameters in both encoder and decoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last GPT2 block\n",
    "for param in model.decoder.transformer.h[11].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# The last layer norm before the LM head\n",
    "for param in model.decoder.transformer.ln_f.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# The last layer norm before the LM head\n",
    "for param in model.decoder.lm_head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48,050,688 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./vit-gpt2-last-block-paras\"\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a121bd1ebb0460cb2c47a7e1d3adfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0813, 'learning_rate': 3.972461981093301e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7647d12830db4d96b310773b8f3418b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5653371810913086, 'eval_runtime': 69.6831, 'eval_samples_per_second': 11.638, 'eval_steps_per_second': 2.913, 'epoch': 1.0}\n",
      "{'loss': 0.5994, 'learning_rate': 2.944923962186601e-05, 'epoch': 1.23}\n",
      "{'loss': 0.5578, 'learning_rate': 1.9173859432799017e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7193cb6cb714775915120b10565a142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5612289309501648, 'eval_runtime': 71.5821, 'eval_samples_per_second': 11.33, 'eval_steps_per_second': 2.836, 'epoch': 2.0}\n",
      "{'loss': 0.5631, 'learning_rate': 8.898479243732018e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fa6d688a5a4f6bbe9bfc8f9c939027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5618223547935486, 'eval_runtime': 70.324, 'eval_samples_per_second': 11.532, 'eval_steps_per_second': 2.887, 'epoch': 3.0}\n",
      "{'train_runtime': 1647.229, 'train_samples_per_second': 5.906, 'train_steps_per_second': 1.477, 'train_loss': 0.6708380106884023, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2433, training_loss=0.6708380106884023, metrics={'train_runtime': 1647.229, 'train_samples_per_second': 5.906, 'train_steps_per_second': 1.477, 'train_loss': 0.6708380106884023, 'epoch': 3.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./vig-gpt2-model-finetuned-on-last-block-paras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1411: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2501: UserWarning: MPS: no support for int64 for min_max, downcasting to a smaller data type (int32/float32). Native support for int64 has been added in macOS 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:621.)\n",
      "  if unfinished_sequences.max() == 0:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I'm so excited to be launching my first app! \",\n",
       " \"I'm so excited to finally be able to share my story with you guys. I'm so\",\n",
       " \"I'm so excited to finally be able to share my story with you guys. I'm so\",\n",
       " \"I'm so excited to finally be able to share my story with you guys. I'm so\",\n",
       " \"I'm so excited to be launching my first app! I'm so excited to be launching my\",\n",
       " 'I love my new lip kit. I love how easy it is to customize my lip kit.',\n",
       " \"I'm so excited to finally be able to share my new collection with you guys! \",\n",
       " \"I'm so excited to be launching my first app! \",\n",
       " \"I'm so excited to be launching my first app! \",\n",
       " \"I'm so excited to be launching my first app! \",\n",
       " \"I'm so excited to finally be able to share my story with you guys. I'm so\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune all trainable parameters\n",
    "- image_encoder_model = \"google/vit-base-patch16-224-in21k\"\n",
    "- text_decode_model = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./vit-gpt2-all-paras\"\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870f7b64731845069079b1271dd471f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6129, 'learning_rate': 3.972461981093301e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c0c5387ee74958b032d81e199d3f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5214453339576721, 'eval_runtime': 72.5733, 'eval_samples_per_second': 11.175, 'eval_steps_per_second': 2.797, 'epoch': 1.0}\n",
      "{'loss': 0.5173, 'learning_rate': 2.944923962186601e-05, 'epoch': 1.23}\n",
      "{'loss': 0.4514, 'learning_rate': 1.9173859432799017e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c990306a9f499b971d5231d582d22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5179643630981445, 'eval_runtime': 70.8282, 'eval_samples_per_second': 11.45, 'eval_steps_per_second': 2.866, 'epoch': 2.0}\n",
      "{'loss': 0.423, 'learning_rate': 8.898479243732018e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d4023fbade4c33a520a82d9ce8151c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.526144802570343, 'eval_runtime': 73.3089, 'eval_samples_per_second': 11.063, 'eval_steps_per_second': 2.769, 'epoch': 3.0}\n",
      "{'train_runtime': 2606.2994, 'train_samples_per_second': 3.733, 'train_steps_per_second': 0.934, 'train_loss': 0.4822222376085337, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2433, training_loss=0.4822222376085337, metrics={'train_runtime': 2606.2994, 'train_samples_per_second': 3.733, 'train_steps_per_second': 0.934, 'train_loss': 0.4822222376085337, 'epoch': 3.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./vig-gpt2-model-finetuned-on-all-paras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I'm so excited to finally reveal my new collection with my new music video coming out tomorrow!\",\n",
       " \"I'm so happy I'm not a vampire \",\n",
       " \"I'm so happy I'm not a vampire \",\n",
       " 'I\\'m a little nervous about this pic but I\\'m not a big fan of the \"I',\n",
       " \"I'm so excited to share my first cover for the new issue of Cosmo! \",\n",
       " \"I'm so happy I'm not a vampire \",\n",
       " \"I'm so happy I got to spend my birthday with my bestie ðŸ’— \",\n",
       " \"I'm so excited to finally share my first collection with you guys! I've been working on\",\n",
       " \"I'm so excited to finally be apart of this family. I love you guys so much.\",\n",
       " \"I'm so happy I'm not a vampire \",\n",
       " \"I'm so happy I'm not a vampire \"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate caption on sample images without fine-tuning on IG dataset\n",
    "- image_encoder_model = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "- text_decode_model = \"nlpconnect/vit-gpt2-image-captioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a fine-tuned image captioning model and corresponding tokenizer and image processor\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# update the model config\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239,195,904 total parameters.\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a large brown and white giraffe standing in a field ',\n",
       " 'a lake with a mountain range and a mountain range ',\n",
       " 'a stuffed bear is in a bear hug ',\n",
       " 'a man standing on a ledge near a river ',\n",
       " 'a painting of a bird with a sky background ',\n",
       " 'a man and woman wearing glasses and sunglasses ',\n",
       " 'a dog wearing a green shirt and a green scarf ',\n",
       " 'a woman with a mask on holding a large knife ',\n",
       " 'a city street with a large building ',\n",
       " 'a blurry photo of a car with a reflection of a water fountain ',\n",
       " 'a large building with a large white building ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "- image_encoder_model = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "- text_decode_model = \"nlpconnect/vit-gpt2-image-captioning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune only the last several layers in the selected model (freeze all the prior layers)\n",
    "- image_encoder_model = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "- text_decode_model = \"nlpconnect/vit-gpt2-image-captioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, freeze all parameters in both encoder and decoder\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last GPT2 block\n",
    "for param in model.decoder.transformer.h[11].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# The last layer norm before the LM head\n",
    "for param in model.decoder.transformer.ln_f.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# The last layer norm before the LM head\n",
    "for param in model.decoder.lm_head.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48,050,688 total parameters.\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_params:,} total parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./nlpconnect-last-block-paras\"\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1427cbe400e4550aa66a8079b54c135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8086, 'learning_rate': 3.972461981093301e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deaf26826d84c0692fb1158f8e5d68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5885778069496155, 'eval_runtime': 73.036, 'eval_samples_per_second': 11.104, 'eval_steps_per_second': 2.779, 'epoch': 1.0}\n",
      "{'loss': 0.6327, 'learning_rate': 2.944923962186601e-05, 'epoch': 1.23}\n",
      "{'loss': 0.5848, 'learning_rate': 1.9173859432799017e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c0d62280044d60ab49934fef923d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5798221230506897, 'eval_runtime': 68.7028, 'eval_samples_per_second': 11.804, 'eval_steps_per_second': 2.955, 'epoch': 2.0}\n",
      "{'loss': 0.5912, 'learning_rate': 8.898479243732018e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c39255f0d54fe1a0c7d9b84639f3e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5791286826133728, 'eval_runtime': 69.0349, 'eval_samples_per_second': 11.748, 'eval_steps_per_second': 2.941, 'epoch': 3.0}\n",
      "{'train_runtime': 1673.2949, 'train_samples_per_second': 5.814, 'train_steps_per_second': 1.454, 'train_loss': 0.6372706942042842, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2433, training_loss=0.6372706942042842, metrics={'train_runtime': 1673.2949, 'train_samples_per_second': 5.814, 'train_steps_per_second': 1.454, 'train_loss': 0.6372706942042842, 'epoch': 3.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./nlpconnect-model-finetuned-on-last-block-paras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I'm not sure what this is about. I'm just curious. \",\n",
       " \"I can't wait to see what the next lake looks like. \",\n",
       " \"I'm not sure what to think of this. I'm just so excited to see this.\",\n",
       " \"I'm not sure what I'm going to do with my life right now. I'm just\",\n",
       " \"I can't wait to see what you guys come up with! \",\n",
       " \"I'm not sure if I'm going to get this one right or not. I'm just\",\n",
       " 'I love this little dog ðŸ’™ðŸ’™ ',\n",
       " \"I'm so excited to see my new favorite \",\n",
       " \"I'm not sure if this is a good place to start my journey. \",\n",
       " \"I can't see the reflection of the water \",\n",
       " \"I can't see the picture above but I can see the building \"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune all trainable parameters\n",
    "- image_encoder_model = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "- text_decode_model = \"nlpconnect/vit-gpt2-image-captioning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    output_dir=\"./vit-gpt2-all-paras\"\n",
    ")\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset['train'],\n",
    "    eval_dataset=processed_dataset['test'],\n",
    "    data_collator=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ee3f2f471e4209be7df3125eb0b2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6084, 'learning_rate': 3.972461981093301e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8328c183666a44cb9b003ad01317ed83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5267587304115295, 'eval_runtime': 72.5089, 'eval_samples_per_second': 11.185, 'eval_steps_per_second': 2.8, 'epoch': 1.0}\n",
      "{'loss': 0.5231, 'learning_rate': 2.944923962186601e-05, 'epoch': 1.23}\n",
      "{'loss': 0.4551, 'learning_rate': 1.9173859432799017e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc66697788fa4d26a60081f0bc9c40ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5228403806686401, 'eval_runtime': 73.5217, 'eval_samples_per_second': 11.031, 'eval_steps_per_second': 2.761, 'epoch': 2.0}\n",
      "{'loss': 0.4226, 'learning_rate': 8.898479243732018e-06, 'epoch': 2.47}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da67b5fd79c4ccd8260517c1109b0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5313527584075928, 'eval_runtime': 75.8511, 'eval_samples_per_second': 10.692, 'eval_steps_per_second': 2.676, 'epoch': 3.0}\n",
      "{'train_runtime': 2721.7336, 'train_samples_per_second': 3.575, 'train_steps_per_second': 0.894, 'train_loss': 0.48252051463599466, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2433, training_loss=0.48252051463599466, metrics={'train_runtime': 2721.7336, 'train_samples_per_second': 3.575, 'train_steps_per_second': 0.894, 'train_loss': 0.48252051463599466, 'epoch': 3.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./nlpconnect-model-finetuned-on-all-paras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stoneman/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1254: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"I'm so excited to share this with you guys! I'm so excited to share this with\",\n",
       " \"I'm going to be back in a few days but this is the best I've had so\",\n",
       " 'ðŸ’— ',\n",
       " \"I'm not sure what this means... I'm just saying that I'm not sure what this\",\n",
       " \"I'm so excited to finally share my first cover for the new Koko edition! I'm\",\n",
       " \"I'm not sure what that means... \",\n",
       " 'ðŸ’œ ',\n",
       " 'ðŸ’œ ',\n",
       " \"I'm not sure what kind of city this is in but it's definitely not me. \",\n",
       " \"I'm not sure what I'm looking at. \",\n",
       " 'ðŸ’— ']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_caption(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embeddings.cls_token torch.Size([1, 1, 768])\n",
      "encoder.embeddings.position_embeddings torch.Size([1, 197, 768])\n",
      "encoder.embeddings.patch_embeddings.projection.weight torch.Size([768, 3, 16, 16])\n",
      "encoder.embeddings.patch_embeddings.projection.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.0.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.0.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.0.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.1.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.1.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.1.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.2.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.2.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.2.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.3.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.3.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.3.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.4.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.4.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.4.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.5.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.5.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.5.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.6.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.6.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.6.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.7.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.7.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.7.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.8.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.8.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.8.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.9.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.9.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.9.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.10.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.10.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.10.layernorm_after.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.attention.query.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.attention.attention.query.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.attention.key.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.attention.attention.key.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.attention.value.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.attention.attention.value.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.layernorm_before.weight torch.Size([768])\n",
      "encoder.encoder.layer.11.layernorm_before.bias torch.Size([768])\n",
      "encoder.encoder.layer.11.layernorm_after.weight torch.Size([768])\n",
      "encoder.encoder.layer.11.layernorm_after.bias torch.Size([768])\n",
      "encoder.layernorm.weight torch.Size([768])\n",
      "encoder.layernorm.bias torch.Size([768])\n",
      "encoder.pooler.dense.weight torch.Size([768, 768])\n",
      "encoder.pooler.dense.bias torch.Size([768])\n",
      "decoder.transformer.wte.weight torch.Size([50257, 768])\n",
      "decoder.transformer.wpe.weight torch.Size([1024, 768])\n",
      "decoder.transformer.h.0.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.0.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.0.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.0.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.0.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.0.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.0.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.0.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.0.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.0.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.0.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.0.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.1.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.1.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.1.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.1.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.1.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.1.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.1.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.1.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.1.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.1.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.1.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.1.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.2.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.2.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.2.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.2.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.2.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.2.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.2.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.2.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.2.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.2.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.2.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.2.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.3.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.3.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.3.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.3.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.3.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.3.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.3.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.3.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.3.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.3.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.3.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.3.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.4.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.4.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.4.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.4.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.4.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.4.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.4.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.4.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.4.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.4.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.4.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.4.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.5.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.5.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.5.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.5.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.5.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.5.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.5.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.5.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.5.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.5.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.5.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.5.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.6.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.6.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.6.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.6.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.6.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.6.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.6.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.6.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.6.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.6.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.6.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.6.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.7.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.7.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.7.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.7.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.7.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.7.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.7.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.7.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.7.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.7.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.7.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.7.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.8.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.8.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.8.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.8.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.8.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.8.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.8.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.8.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.8.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.8.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.8.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.8.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.9.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.9.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.9.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.9.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.9.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.9.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.9.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.9.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.9.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.9.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.9.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.9.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.10.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.10.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.10.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.10.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.10.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.10.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.10.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.10.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.10.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.10.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.10.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.10.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.11.ln_1.weight torch.Size([768])\n",
      "decoder.transformer.h.11.ln_1.bias torch.Size([768])\n",
      "decoder.transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "decoder.transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "decoder.transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.11.ln_2.weight torch.Size([768])\n",
      "decoder.transformer.h.11.ln_2.bias torch.Size([768])\n",
      "decoder.transformer.h.11.crossattention.c_attn.weight torch.Size([768, 1536])\n",
      "decoder.transformer.h.11.crossattention.c_attn.bias torch.Size([1536])\n",
      "decoder.transformer.h.11.crossattention.q_attn.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.11.crossattention.q_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.11.crossattention.c_proj.weight torch.Size([768, 768])\n",
      "decoder.transformer.h.11.crossattention.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.h.11.ln_cross_attn.weight torch.Size([768])\n",
      "decoder.transformer.h.11.ln_cross_attn.bias torch.Size([768])\n",
      "decoder.transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "decoder.transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "decoder.transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "decoder.transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "decoder.transformer.ln_f.weight torch.Size([768])\n",
      "decoder.transformer.ln_f.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# # check parameters in each layer\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fine/preprocessor_config.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"fine\"\n",
    "feature_extractor.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_examples(image_paths, generated_texts, size=(350, 350)):\n",
    "#     w, h = size\n",
    "#     grid_width = w * 3\n",
    "#     grid_height = h * 3\n",
    "#     grid = Image.new('RGB', size=(grid_width, grid_height))\n",
    "#     draw = ImageDraw.Draw(grid)\n",
    "\n",
    "\n",
    "#     for idx, (image_path, text) in enumerate(zip(image_paths, generated_texts)):\n",
    "#         image = Image.open(image_path)\n",
    "#         box = ((idx % 3) * w, (idx // 3) * h)\n",
    "#         grid.paste(image.resize(size), box=box)\n",
    "#         # Draw the label text\n",
    "#         text_position = (box[0], box[1] + h - 20)  # Adjust text position as needed\n",
    "#         draw.text(text_position, text, (255, 255, 255))  # Use the emoji-compatible font\n",
    "\n",
    "#     return grid\n",
    "\n",
    "# show_examples(image_paths, generated_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
